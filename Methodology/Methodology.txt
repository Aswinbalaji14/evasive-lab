ðŸ§ª Methodology
The Evasive AI Lab employs a rigorous, automated red-teaming workflow designed to quantify the adversarial resilience of Large Language Models (LLMs) against standardized attack vectors. Our approach is built on three core pillars:

Standardized Frameworks: We utilize the garak (Generative AI Red-teaming & Assessment Kit) framework to ensure reproducibility and consistency. By leveraging a structured "Probe-Detector" architecture, we move beyond anecdotal "chat-based" testing to systematic vulnerability discovery.

Statistical Auditing: To account for the inherent non-determinism of generative models, we perform 256 generations per probe. This high-volume sampling allows us to calculate a statistically significant Attack Success Rate (ASR), distinguishing consistent safety failures from statistical outliers.

NIST AML Alignment: Every probe and identified vulnerability is mapped directly to the NIST AI 100-2e2025 taxonomy. This ensures that our findings are actionable within a global regulatory and compliance context, specifically targeting Direct Prompt Injection (NISTAML.018) and Adversarial Evasion (NISTAML.017).
